Homework \#6
================
Robert Tumasian (rat2134)
11/19/2019

``` r
#Load required packages
library(tidyverse)
library(modelr)
library(mgcv)
```

# Problem 1

``` r
#Import and clean data
birthweight_data = 
  read.csv("./hw6_data/birthweight.csv") %>%
  mutate(babysex = factor(babysex),
         frace = factor(frace),
         malform = factor(malform),
         mrace = factor(mrace))

#Check for missing data
length(which(is.na(birthweight_data))) 
```

    ## [1] 0

All categorical variables were converted to factors and there are no
missing values in the `birthweight` dataset. Now, we will identify an
most appropriate regression model for birthweight. We will hypothesize
that the following variables underly birthweight:

  - `babysex`: the baby’s sex
  - `blength`: the baby’s length
  - `fincome`: family income
  - `gaweeks`: gestational age (in weeks)
  - `malform`: the presence of malformations
  - `momage`: the mother’s age
  - `smoken`: average number of cigarettes smoked per day during
    pregnancy

We will use each of these variables in our initial regression model for
birthweight.

``` r
#Regression model
birthweight_reg_model = lm(bwt ~ babysex + blength + fincome + gaweeks + 
                             malform + momage + smoken, 
                           data = birthweight_data)

summary(birthweight_reg_model)
```

    ## 
    ## Call:
    ## lm(formula = bwt ~ babysex + blength + fincome + gaweeks + malform + 
    ##     momage + smoken, data = birthweight_data)
    ## 
    ## Residuals:
    ##     Min      1Q  Median      3Q     Max 
    ## -1669.8  -214.0   -14.2   203.5  4119.2 
    ## 
    ## Coefficients:
    ##               Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept) -4353.3055    99.3797 -43.805  < 2e-16 ***
    ## babysex2      -17.4441    10.0823  -1.730   0.0837 .  
    ## blength       126.3491     1.9907  63.468  < 2e-16 ***
    ## fincome         1.4632     0.2042   7.165 9.10e-13 ***
    ## gaweeks        26.0012     1.7131  15.178  < 2e-16 ***
    ## malform1       80.5029    85.1990   0.945   0.3448    
    ## momage          5.6066     1.3693   4.094 4.31e-05 ***
    ## smoken         -3.2564     0.6810  -4.782 1.80e-06 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 329.1 on 4334 degrees of freedom
    ## Multiple R-squared:  0.5878, Adjusted R-squared:  0.5871 
    ## F-statistic: 882.9 on 7 and 4334 DF,  p-value: < 2.2e-16

From our regression results above, we can see that `babysex` and
`malform` are not significantly associated with birthweight (`bwt`),
since `p < 0.05`. Therefore, we can remove these variables from the
model.

``` r
#New regression model
new_birthweight_reg_model = lm(bwt ~ blength + fincome + gaweeks + momage + smoken, 
                           data = birthweight_data)

summary(new_birthweight_reg_model)
```

    ## 
    ## Call:
    ## lm(formula = bwt ~ blength + fincome + gaweeks + momage + smoken, 
    ##     data = birthweight_data)
    ## 
    ## Residuals:
    ##     Min      1Q  Median      3Q     Max 
    ## -1666.3  -214.2   -15.3   203.5  4139.5 
    ## 
    ## Coefficients:
    ##               Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept) -4375.7786    98.5022 -44.423  < 2e-16 ***
    ## blength       126.7583     1.9764  64.137  < 2e-16 ***
    ## fincome         1.4630     0.2042   7.163 9.21e-13 ***
    ## gaweeks        25.7925     1.7095  15.087  < 2e-16 ***
    ## momage          5.6994     1.3688   4.164 3.19e-05 ***
    ## smoken         -3.1931     0.6804  -4.693 2.77e-06 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 329.2 on 4336 degrees of freedom
    ## Multiple R-squared:  0.5874, Adjusted R-squared:  0.5869 
    ## F-statistic:  1235 on 5 and 4336 DF,  p-value: < 2.2e-16

After removing `babysex` and `malform`, the adjusted R-squared value
changes negligibly, meaning that these two variables did not explain any
further variance in the outcome that was not already accounted for by
the other variables. In other words, these two variables were not
substantially improving our model, so removing them was appropriate.

``` r
#Plot of model residuals against fitted values
add_residuals(birthweight_data, new_birthweight_reg_model) %>%
  add_predictions(new_birthweight_reg_model) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  labs(
    title = "Residuals vs. Fitted Values",
    x = "Fitted Birthweight (grams)",
    y = "Model Residuals"
  ) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red")
```

![](p8105_HW6_rat2134_files/figure-gfm/unnamed-chunk-5-1.png)<!-- -->

Not all of the model residuals seem to be scattered randomly around `y
= 0` and outliers are present. Therefore, one or more of the variables
in the model may not have a linear relationship with the outcome of
interest (birthweight). Transforming the birthweight variable or
utilizing formal criteria for outlier removal may foster the development
of a stronger model.

``` r
#Model comparison #1
#Using length at birth and gestational age as predictors (main effects only)
comp_model_1 = lm(bwt ~ blength + gaweeks, data = birthweight_data)
```

``` r
#Model comparison #2
#Using head circumference, length, sex, and all interactions (including three-way)
comp_model_2 = lm(bwt ~ (bhead + blength + babysex)^3, data = birthweight_data)
```

``` r
set.seed(144)

#Cross-validation
cv_data = 
  crossv_mc(birthweight_data, 100)

cv_data = 
  cv_data %>% 
  mutate(initial_model  = map(train, ~new_birthweight_reg_model),
         model_comp_1 = map(train, ~comp_model_1),
         model_comp_2  = map(train, ~comp_model_2)) %>% 
  mutate(rmse_initial_model = map2_dbl(initial_model, test, ~rmse(model = .x, data = .y)),
         rmse_model_comp_1 = map2_dbl(model_comp_1, test, ~rmse(model = .x, data = .y)),
         rmse_model_comp_2 = map2_dbl(model_comp_2, test, ~rmse(model = .x, data = .y)))

#Plot of the prediction error distribution for each model
cv_data %>% 
  select(starts_with("rmse")) %>% 
pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin() +
  scale_x_discrete(labels=c("initial_model" = "Initial", 
                            "model_comp_1" = "Comparison 1",
                            "model_comp_2" = "Comparison 2")) +
  labs(
    title = "Prediction error distribution for each model",
    x = "Model",
    y = "RMSE"
  )
```

![](p8105_HW6_rat2134_files/figure-gfm/unnamed-chunk-8-1.png)<!-- -->

We can see that, on average, Comparison 2 (the model containing head
circumference, length, sex, and all interactions) has the lowest RMSE
and is therefore the strongest model.

# Problem 2
