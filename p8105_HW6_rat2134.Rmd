---
title: "Homework #6"
author: "Robert Tumasian (rat2134)"
date: "11/19/2019"
output: github_document
---
```{r, message=FALSE}
#Load required packages
library(tidyverse)
library(modelr)
library(mgcv)
```

# Problem 1
```{r}
#Import and clean data
birthweight_data = 
  read.csv("./hw6_data/birthweight.csv") %>%
  mutate(babysex = factor(babysex),
         frace = factor(frace),
         malform = factor(malform),
         mrace = factor(mrace))

#Check for missing data
length(which(is.na(birthweight_data))) 
```

All categorical variables were converted to factors and there are no missing values in the `birthweight` dataset. Now, we will identify an most appropriate regression model for birthweight. We will hypothesize that the following variables underly birthweight:  

* `babysex`: the baby's sex
* `blength`: the baby's length
* `fincome`: family income
* `gaweeks`: gestational age (in weeks)
* `malform`: the presence of malformations
* `momage`: the mother's age
* `smoken`: average number of cigarettes smoked per day during pregnancy

We will use each of these variables in our initial regression model for birthweight.
```{r}
#Regression model
birthweight_reg_model = lm(bwt ~ babysex + blength + fincome + gaweeks + 
                             malform + momage + smoken, 
                           data = birthweight_data)

summary(birthweight_reg_model)
```

From our regression results above, we can see that `babysex` and `malform` are not significantly associated with birthweight (`bwt`), since `p < 0.05`. Therefore, we can remove these variables from the model.

```{r}
#New regression model
new_birthweight_reg_model = lm(bwt ~ blength + fincome + gaweeks + momage + smoken, 
                           data = birthweight_data)

summary(new_birthweight_reg_model)
```

After removing `babysex` and `malform`, the adjusted R-squared value changes negligibly, meaning that these two variables did not explain any further variance in the outcome that was not already accounted for by the other variables. In other words, these two variables were not substantially improving our model, so removing them was appropriate.

```{r}
#Plot of model residuals against fitted values
add_residuals(birthweight_data, new_birthweight_reg_model) %>%
  add_predictions(new_birthweight_reg_model) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  labs(
    title = "Residuals vs. Fitted Values",
    x = "Fitted Birthweight (grams)",
    y = "Model Residuals"
  ) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red")

```

Not all of the model residuals seem to be scattered randomly around `y = 0` and outliers are present. Therefore, one or more of the variables in the model may not have a linear relationship with the outcome of interest (birthweight). Transforming the birthweight variable or implementing formal criteria for outlier removal may foster the development of a stronger model.

```{r}
#Model comparison #1
#Using length at birth and gestational age as predictors (main effects only)
comp_model_1 = lm(bwt ~ blength + gaweeks, 
                  data = birthweight_data)
```

```{r}
#Model comparison #2
#Using head circumference, length, sex, and all interactions (including three-way)
comp_model_2 = lm(bwt ~ (bhead + blength + babysex)^3, 
                  data = birthweight_data)
```

```{r}
set.seed(144)

#Cross-validation
cv_data = 
  crossv_mc(birthweight_data, 100)

cv_data = 
  cv_data %>% 
  mutate(initial_model  = map(train, ~new_birthweight_reg_model),
         model_comp_1 = map(train, ~comp_model_1),
         model_comp_2  = map(train, ~comp_model_2)) %>% 
  mutate(rmse_initial_model = map2_dbl(initial_model, test, ~rmse(model = .x, data = .y)),
         rmse_model_comp_1 = map2_dbl(model_comp_1, test, ~rmse(model = .x, data = .y)),
         rmse_model_comp_2 = map2_dbl(model_comp_2, test, ~rmse(model = .x, data = .y)))

#Plot of the prediction error distribution for each model
cv_data %>% 
  select(starts_with("rmse")) %>% 
pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin() +
  scale_x_discrete(labels=c("initial_model" = "Initial", 
                            "model_comp_1" = "Comparison 1",
                            "model_comp_2" = "Comparison 2")) +
  labs(
    title = "Prediction error distribution for each model",
    x = "Model",
    y = "RMSE"
  )
```

We can see that, on average, model comparison #2 (the model containing head circumference, length, sex, and all interactions) has the lowest RMSE and is therefore the strongest model.

# Problem 2
```{r, message=FALSE}
#Load data
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

```{r, warning=FALSE}
set.seed(223)

#Bootstrap samples
boot_straps = 
  data_frame(
    strap_number = 1:5000,
    strap_sample = rerun(5000, sample_frac(weather_df, replace = TRUE)))

#Bootstrap results
bootstrap_results = 
  boot_straps %>% 
  mutate(
    models = map(strap_sample, ~lm(tmax ~ tmin, data = .x)),
    results_r2 = map(models, broom::glance),
    results_estimates = map(models, broom::tidy)) %>% 
  select(-strap_sample, -models) %>% 
  unnest() %>%
  select(strap_number, r.squared, term, estimate) %>%
  pivot_wider(
    names_from = "term",
    values_from = "estimate"
  ) %>%
  janitor::clean_names() %>%
  mutate(log_est = log(intercept * tmin))
```

```{r}
#Confidence intervals for estimates
quantile(pull(bootstrap_results, r_squared), probs = c(0.025, 0.975))
quantile(pull(bootstrap_results, log_est), probs = c(0.025, 0.975))
```

Therefore, a 95% confidence interval for the estimated coefficient of determination is (0.894, 0.927). A 95% confidence interval for the log of the product of the beta estimates is (1.965, 2.059).

```{r}
#Plotting distribution of estimates
ggplot(data = bootstrap_results, aes(x = r_squared)) +
  geom_histogram()

ggplot(data = bootstrap_results, aes(x = log_est)) +
  geom_histogram()
```